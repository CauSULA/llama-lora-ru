{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c5671f6-83f2-4b9b-b3d4-85c58fbe4df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers (from -r requirements.txt (line 1))\n",
      "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-kmd9qu1v\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-kmd9qu1v\n",
      "  Resolved https://github.com/huggingface/transformers to commit 312b104ff65514736c0475814fec19e47425b0b5\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting datasets\n",
      "  Using cached datasets-2.12.0-py3-none-any.whl (474 kB)\n",
      "Collecting jsonlines\n",
      "  Using cached jsonlines-3.1.0-py3-none-any.whl (8.6 kB)\n",
      "Requirement already satisfied: zstandard in /home/user/conda/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (0.19.0)\n",
      "Collecting peft\n",
      "  Downloading peft-0.3.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m622.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting accelerate\n",
      "  Using cached accelerate-0.18.0-py3-none-any.whl (215 kB)\n",
      "Collecting bitsandbytes\n",
      "  Using cached bitsandbytes-0.38.1-py3-none-any.whl (104.3 MB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/user/conda/lib/python3.9/site-packages (from transformers==4.29.0.dev0->-r requirements.txt (line 1)) (4.64.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/user/conda/lib/python3.9/site-packages (from transformers==4.29.0.dev0->-r requirements.txt (line 1)) (2022.10.31)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/user/conda/lib/python3.9/site-packages (from transformers==4.29.0.dev0->-r requirements.txt (line 1)) (1.23.5)\n",
      "Requirement already satisfied: requests in /home/user/conda/lib/python3.9/site-packages (from transformers==4.29.0.dev0->-r requirements.txt (line 1)) (2.28.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/user/conda/lib/python3.9/site-packages (from transformers==4.29.0.dev0->-r requirements.txt (line 1)) (22.0)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0\n",
      "  Using cached huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "Requirement already satisfied: filelock in /home/user/conda/lib/python3.9/site-packages (from transformers==4.29.0.dev0->-r requirements.txt (line 1)) (3.9.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/user/conda/lib/python3.9/site-packages (from transformers==4.29.0.dev0->-r requirements.txt (line 1)) (6.0)\n",
      "Collecting fsspec[http]>=2021.11.1\n",
      "  Using cached fsspec-2023.4.0-py3-none-any.whl (153 kB)\n",
      "Collecting responses<0.19\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home/user/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 2)) (0.3.6)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/user/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 2)) (10.0.1)\n",
      "Requirement already satisfied: pandas in /home/user/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 2)) (1.5.3)\n",
      "Collecting multiprocess\n",
      "  Using cached multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
      "Collecting xxhash\n",
      "  Using cached xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "Requirement already satisfied: aiohttp in /home/user/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 2)) (3.8.3)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /home/user/conda/lib/python3.9/site-packages (from jsonlines->-r requirements.txt (line 3)) (22.2.0)\n",
      "Requirement already satisfied: psutil in /home/user/conda/lib/python3.9/site-packages (from peft->-r requirements.txt (line 5)) (5.9.4)\n",
      "Requirement already satisfied: torch>=1.13.0 in /home/user/conda/lib/python3.9/site-packages (from peft->-r requirements.txt (line 5)) (2.0.0.dev20230205+cu117)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/user/conda/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.8.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/user/conda/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (6.0.4)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/user/conda/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (2.1.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/user/conda/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/user/conda/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/user/conda/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (4.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/user/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.29.0.dev0->-r requirements.txt (line 1)) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/user/conda/lib/python3.9/site-packages (from requests->transformers==4.29.0.dev0->-r requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/user/conda/lib/python3.9/site-packages (from requests->transformers==4.29.0.dev0->-r requirements.txt (line 1)) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/user/conda/lib/python3.9/site-packages (from requests->transformers==4.29.0.dev0->-r requirements.txt (line 1)) (1.26.14)\n",
      "Requirement already satisfied: sympy in /home/user/conda/lib/python3.9/site-packages (from torch>=1.13.0->peft->-r requirements.txt (line 5)) (1.11.1)\n",
      "Requirement already satisfied: networkx in /home/user/conda/lib/python3.9/site-packages (from torch>=1.13.0->peft->-r requirements.txt (line 5)) (3.0)\n",
      "Requirement already satisfied: pytorch-triton==2.0.0+0d7e753227 in /home/user/conda/lib/python3.9/site-packages (from torch>=1.13.0->peft->-r requirements.txt (line 5)) (2.0.0+0d7e753227)\n",
      "Requirement already satisfied: cmake in /home/user/conda/lib/python3.9/site-packages (from pytorch-triton==2.0.0+0d7e753227->torch>=1.13.0->peft->-r requirements.txt (line 5)) (3.25.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/user/conda/lib/python3.9/site-packages (from pandas->datasets->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/user/conda/lib/python3.9/site-packages (from pandas->datasets->-r requirements.txt (line 2)) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/user/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/user/conda/lib/python3.9/site-packages (from sympy->torch>=1.13.0->peft->-r requirements.txt (line 5)) (1.2.1)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.29.0.dev0-py3-none-any.whl size=7012297 sha256=8726fb3f6549005c09dc522ee7eacc990dd27ba390f5c880bf43e56cfc437547\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-05oxyent/wheels/14/a0/7b/8f6b25ba4110aa215fcb8d6aedd6cd4f9b9b6619190999ac2b\n",
      "Successfully built transformers\n",
      "Installing collected packages: tokenizers, bitsandbytes, xxhash, multiprocess, jsonlines, fsspec, responses, huggingface-hub, transformers, datasets, accelerate, peft\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/home/jovyan/.imgenv-unruffled-hypatia-0/lib/python3.9/site-packages/transformers-4.29.0.dev0.dist-info/INSTALLERdimvvsy3.tmp'\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00216ea5-eb8b-4029-b75c-09031c970af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32d09df0-8171-4736-b1e3-46db7e684cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaTokenizer\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"hf-internal-testing/llama-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad546fe6-15a0-4ddc-888e-0c7a56ade17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: rulm/default\n",
      "Found cached dataset rulm (/home/jovyan/.cache/huggingface/datasets/IlyaGusev___rulm/default/0.0.1/c94da05286e8839c1219f9a5061c630709a487fb298427a6f5a73ee0fe2cd2cb)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "dataset = load_from_disk(\"./chunk_dataset\")\n",
    "val_dataset = load_dataset(\"IlyaGusev/rulm\", split=\"validation[:10%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87bff9dc-2783-41f7-a9b7-2f22a8f64402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3af748bd-b623-4751-bad3-f9d55da0520d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(prompt, add_eos_token=True):\n",
    "    result = tokenizer(prompt)\n",
    "    if (\n",
    "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "        and add_eos_token\n",
    "    ):\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f097ec21-20f4-4420-8e6b-6d912f8c97bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "def chunk_examples(examples, max_length):\n",
    "    input_ids_chunks = []\n",
    "    attention_mask_chunks = []\n",
    "    # print(len(examples[\"input_ids\"]))\n",
    "    input_ids = list(chain(*examples[\"input_ids\"]))\n",
    "    attention_mask = list(chain(*examples[\"attention_mask\"]))\n",
    "    input_ids_chunks.extend(\n",
    "        [\n",
    "            input_ids[i : i + max_length]\n",
    "            for i in range(0, len(input_ids), max_length)\n",
    "        ]\n",
    "    )\n",
    "    attention_mask_chunks.extend(\n",
    "        [\n",
    "            attention_mask[i : i + max_length]\n",
    "            for i in range(0, len(attention_mask), max_length)\n",
    "        ]\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": input_ids_chunks,\n",
    "        \"attention_mask\": attention_mask_chunks,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e08d64a4-2847-44f6-9c0b-d667f11898fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset):\n",
    "    tokenized_dataset = dataset.map(lambda x: tokenize(x['text']),\n",
    "                                    num_proc=16, remove_columns=[\"meta\"])\n",
    "    chunk_dataset = tokenized_dataset.map(lambda x: chunk_examples(x, 512), batched=True, batch_size=100,\n",
    "                                          num_proc=16, remove_columns=tokenized_dataset.column_names)\n",
    "    return tokenized_dataset, chunk_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f2d9639-e97e-4fdf-801c-25cc7a3fdba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/IlyaGusev___rulm/default/0.0.1/c94da05286e8839c1219f9a5061c630709a487fb298427a6f5a73ee0fe2cd2cb/cache-f418d707dbed0c01_*_of_00016.arrow\n",
      "Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/IlyaGusev___rulm/default/0.0.1/c94da05286e8839c1219f9a5061c630709a487fb298427a6f5a73ee0fe2cd2cb/cache-826a7ac955c9b816_*_of_00016.arrow\n"
     ]
    }
   ],
   "source": [
    "_, chunk_val_dataset = prepare_dataset(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb0505ee-8625-4443-bd0f-08dcc41ff5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bd5e56c-5227-4285-a4e2-eec6a49512bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/jovyan/.imgenv-heuristic-hypatia-0/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/jovyan/.imgenv-heuristic-hypatia-0/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.imgenv-heuristic-hypatia-0/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /home/jovyan/.imgenv-heuristic-hypatia-0 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/jovyan/.imgenv-heuristic-hypatia-0/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\n",
      "/home/jovyan/.imgenv-heuristic-hypatia-0/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/jovyan/.imgenv-heuristic-hypatia-0/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('https'), PosixPath('//download.openmmlab.com/mmcv/dist/cu117/torch1.11.0/index.html')}\n",
      "  warn(msg)\n",
      "/home/jovyan/.imgenv-heuristic-hypatia-0/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('tcp'), PosixPath('//172.18.215.105'), PosixPath('80')}\n",
      "  warn(msg)\n",
      "/home/jovyan/.imgenv-heuristic-hypatia-0/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//172.18.110.92'), PosixPath('tcp'), PosixPath('22')}\n",
      "  warn(msg)\n",
      "/home/jovyan/.imgenv-heuristic-hypatia-0/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//172.18.77.205'), PosixPath('tcp'), PosixPath('22')}\n",
      "  warn(msg)\n",
      "/home/jovyan/.imgenv-heuristic-hypatia-0/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sr002-mt/notebook/ai0002258-00001/heuristic-hypatia')}\n",
      "  warn(msg)\n",
      "/home/jovyan/.imgenv-heuristic-hypatia-0/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('Europe/Moscow')}\n",
      "  warn(msg)\n",
      "/home/jovyan/.imgenv-heuristic-hypatia-0/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//172.18.110.92'), PosixPath('tcp'), PosixPath('80')}\n",
      "  warn(msg)\n",
      "/home/jovyan/.imgenv-heuristic-hypatia-0/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('tcp'), PosixPath('443'), PosixPath('//172.18.0.1')}\n",
      "  warn(msg)\n",
      "/home/jovyan/.imgenv-heuristic-hypatia-0/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//172.18.77.205'), PosixPath('tcp'), PosixPath('80')}\n",
      "  warn(msg)\n",
      "/home/jovyan/.imgenv-heuristic-hypatia-0/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('0.0.33'), PosixPath('cr.msk.sbercloud.ru/aicloud-base-images/cuda11.7-pt2')}\n",
      "  warn(msg)\n",
      "/home/jovyan/.imgenv-heuristic-hypatia-0/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('https'), PosixPath('//download.pytorch.org/whl/torch_stable.html')}\n",
      "  warn(msg)\n",
      "/home/jovyan/.imgenv-heuristic-hypatia-0/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('22'), PosixPath('tcp'), PosixPath('//172.18.215.105')}\n",
      "  warn(msg)\n",
      "/home/jovyan/.imgenv-heuristic-hypatia-0/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('cr.msk.sbercloud.ru/aicloud-jupyter-test/jupyter-cuda11.7-pt2'), PosixPath('cbfa7664-374246')}\n",
      "  warn(msg)\n",
      "/home/jovyan/.imgenv-heuristic-hypatia-0/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/home/jovyan/.imgenv-heuristic-hypatia-0/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea197b90d3441719ed832325a10bb37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig, prepare_model_for_int8_training\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"decapoda-research/llama-7b-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"decapoda-research/llama-7b-hf\", load_in_8bit=True, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eacdb700-a1e9-4640-9c67-70b608f261ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc = {\n",
    "        \"r\": 8,\n",
    "        \"lora_alpha\": 16,\n",
    "        \"lora_dropout\": 0.05,\n",
    "        \"bias\": \"none\",\n",
    "        \"target_modules\": [\"q_proj\", \"v_proj\"],\n",
    "        \"task_type\": \"CAUSAL_LM\"\n",
    "    }\n",
    "\n",
    "lora_config = LoraConfig(**lc)\n",
    "model = prepare_model_for_int8_training(model, use_gradient_checkpointing=False)\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f3bd51d-7079-4fce-a6be-7718c33a81cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 4096, padding_idx=31999)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear8bitLt(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear8bitLt(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): CastOutputToFloat(\n",
       "        (0): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1717a295-664e-409c-b345-de7de36b41e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"trainer\": {\n",
    "#     \"evaluation_strategy\": \"steps\",\n",
    "#     \"per_device_train_batch_size\": 16,\n",
    "#     \"per_device_eval_batch_size\": 16,\n",
    "#     \"gradient_accumulation_steps\": 8,\n",
    "#     \"eval_steps\": 75,\n",
    "#     \"save_steps\": 75,\n",
    "#     \"logging_steps\": 5,\n",
    "#     \"learning_rate\": 0.0003,\n",
    "#     \"num_train_epochs\": 3,\n",
    "#     \"lr_scheduler_type\": \"cosine\",\n",
    "#     \"warmup_steps\": 50,\n",
    "#     \"fp16\": true,\n",
    "#     \"bf16\": false,\n",
    "#     \"torch_compile\": false,\n",
    "#     \"optim\": \"adamw_torch\"\n",
    "# },\n",
    "training_args = TrainingArguments(\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=75,\n",
    "    eval_steps=75,\n",
    "    warmup_steps=50,\n",
    "    logging_steps=5,\n",
    "\n",
    "    gradient_accumulation_steps=16,\n",
    "\n",
    "    num_train_epochs=1,\n",
    "\n",
    "    learning_rate=0.0003,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "\n",
    "    per_device_eval_batch_size=4,\n",
    "    per_device_train_batch_size=4,\n",
    "    \n",
    "    gradient_checkpointing=False,\n",
    "\n",
    "    fp16=True,\n",
    "    # bf16=False,\n",
    "    torch_compile=False,\n",
    "    optim=\"adamw_torch\",\n",
    "\n",
    "    logging_dir='./logs',\n",
    "    output_dir='./trained',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=chunk_val_dataset,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb9060ed-c942-40eb-aa2c-75e27c378ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='54876' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    9/54876 01:28 < 192:32:08, 0.08 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.imgenv-heuristic-hypatia-0/lib/python3.9/site-packages/transformers/trainer.py:1662\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1657\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1659\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1661\u001b[0m )\n\u001b[0;32m-> 1662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.imgenv-heuristic-hypatia-0/lib/python3.9/site-packages/transformers/trainer.py:1930\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1928\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1929\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1930\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1933\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1934\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1935\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1936\u001b[0m ):\n\u001b[1;32m   1937\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1938\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.imgenv-heuristic-hypatia-0/lib/python3.9/site-packages/transformers/trainer.py:2710\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2707\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n\u001b[1;32m   2709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_grad_scaling:\n\u001b[0;32m-> 2710\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2711\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_apex:\n\u001b[1;32m   2712\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m amp\u001b[38;5;241m.\u001b[39mscale_loss(loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer) \u001b[38;5;28;01mas\u001b[39;00m scaled_loss:\n",
      "File \u001b[0;32m/home/user/conda/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/user/conda/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f05b1295-a413-489d-b61e-77ce600c790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "117aa9f9-c7dd-42de-acfe-b8dc675f6949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a26930ec-159e-4389-bb47-7ca2af89d05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dd7aef67-1293-4640-850a-78e828b51720",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_collator([dataset[0], dataset[1]])['input_ids'].to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b342f233-17f8-4c02-b066-d52e0600b836",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_ids = cuda_m(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ed2beb7d-5d2c-43ac-ae41-bd2adf7782c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_ids.logits.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cad68dfe-d2f8-4b3b-8126-0b2d522884f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.CausalLMOutputWithPast"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(generate_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9389e9a5-d5e3-4439-ae4e-6c15d20f8d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "del generate_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1bd1136f-e8cb-4917-86d9-10eb5f9392ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04b4bf67-4d6a-400f-b9e9-85ab6ade7a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'past_key_values'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_ids.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
